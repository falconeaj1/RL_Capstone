{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed23d95-7b37-4663-b316-b23f49762e73",
   "metadata": {},
   "source": [
    "# Borrowed ideas from https://github.com/nuno-faria/tetris-ai/blob/master/README.md\n",
    "## Including:\n",
    "### - what properties to look for in a board (lines about to be cleared), holes, bumpiness, total height (added next piece preview and swap piece, and coded on own)\n",
    "### - how to set up and run training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2ecf1-3b95-4da1-8e03-2dccc2bed204",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RL Tetris Project Outline:\n",
    "0) Clean up code\n",
    "    - separate classes from main loop and RL components (easier to run 1 cell at the moment)\n",
    "    - remove legacy comments\n",
    "    - make variables any magic numbers (mainly coordinates)\n",
    "    - make production ready\n",
    "1) [x] Create figure generator\n",
    "    - [x] Choose one of 7 blocks (O, I, S, Z, J, L, T)\n",
    "        - [x] Figure out color for block\n",
    "        - [x] List out rotations for each block\n",
    "2) Generate board/figure\n",
    "    - [x] Adjust general layout for board\n",
    "        - [x] timer\n",
    "        - [x] score display\n",
    "        - [x] lines display\n",
    "        - level display \n",
    "    - Implement buttons (not essential atm)\n",
    "        - new game button (esc for now)\n",
    "        - reset button\n",
    "        - pause button\n",
    "    - Implement debugger/ replay buff\n",
    "        - back (last n = 5 blocks, and game states, probably useful for debugging)\n",
    "        * need to store matrix, queue, swap piece and time? Reset all. Not sure how to deal with other saved states?\n",
    "    - [x] area for next blocks (show up to k=3)\n",
    "    - [x] area for swapping blocks\n",
    "3) Game logic\n",
    "    - clean up timer for piece movement\n",
    "    - [x] set up block shadow where piece will land (useful for computer mode)\n",
    "    - Set up controls for piece movement\n",
    "        - [x] human mode: button presses\n",
    "        - [x] computer mode (render or not)\n",
    "    - [x] make piece generator function\n",
    "        - [x] go to next piece queue to display\n",
    "        - [x] previous next piece gets bumped to board (above game)\n",
    "    - [x] make movement functions\n",
    "        - [x] left, right, soft drop, hard drop\n",
    "        - [x] CW rotate, CCW rotate\n",
    "        - [x] kick checks for rotations (only checked example shown here: https://tetris.fandom.com/wiki/SRShttps://tetris.fandom.com/wiki/SRS, HARD to check (I'm not a skilled Tetris player)!\n",
    "    - update screen function\n",
    "        - [x] move piece\n",
    "        - check for silver/golden squares\n",
    "        - check for line clears\n",
    "            - [x] drop pieces above (already implemented in original code)\n",
    "            - [x] adjust score (could be updated)\n",
    "            - [x] adjust lines\n",
    "            - [x] adjust level\n",
    "            - play sound\n",
    "           \n",
    "4) RL Algorithm\n",
    "    - review articles on making tetris and how to avoid getting stuck with delayed reward\n",
    "    - research if possible to help model train by giving examples (showing how to clear lines, probably slow)\n",
    "    - [x] create option for human input and computer with rendering (allow policy to be chosen, random or model)\n",
    "        - [x] Make computer play random moves sampling from action space of allowable moves\n",
    "        - Eventually swap to play best model moves and render\n",
    "        - Develop algorithm to figure out 'all' (maybe most since getting every landing seems super difficult) landing spots and moves to get there\n",
    "    - Create OpenAI gym tetris environment\n",
    "    - Train model\n",
    "        - Test different action spaces\n",
    "            - Simple movements\n",
    "            - complex movements\n",
    "        - test different algorithms\n",
    "            - double q\n",
    "            - PPO\n",
    "            - add more here...\n",
    "        - test if adding heuristics as features improves convergence\n",
    "    - PROBABLY MUCH MORE TO DO IN THIS SECTION\n",
    "5) Streamlit App bonus\n",
    "    - allow user to play vs computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892aa55-0513-4787-a73a-99169a211ac1",
   "metadata": {},
   "source": [
    "## To manually move around next pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2cc23a-731f-4bd8-9788-aedc83672755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import tetris_environment\n",
    "tetris_env = tetris_environment.Tetris_Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0cfa84-a7a5-44c6-ac7e-f1cb4afa4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = tetris_env()\n",
    "game.reset(render_mode = 'human');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c2a211-ce59-449a-8cab-e014e09853be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things for testing:\n",
    "# game.step(action_dict={'swap': 0, 'rotation': 0, 'shift': 0});\n",
    "# game.do_naive_action(action = game.actions['swap'])\n",
    "# game.game.get_properties()\n",
    "# game.game.get_next_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e17024-c327-4bf7-91e2-c54d4e5071d5",
   "metadata": {},
   "source": [
    "# TO PLAY THE GAME: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47be6c3f-2153-4873-86f5-6031bf093b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game.play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b3cc6-9a20-48ca-840a-8444fba1ff98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f172152-1807-4658-b296-01632b7590c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129997f-66f1-483d-99f1-6e5ad5f9984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP NEURAL NETWORK TO PLAY GAME\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from tensorflow.keras import Sequential\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "#  code modified from https://keon.github.io/deep-q-learning/\n",
    "# and  https://github.com/nuno-faria/tetris-ai/blob/master/README.md\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.995    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        # print(f'input shape = {(self.state_size,)}')\n",
    "        # print(f'output shape = {self.action_size}')\n",
    "\n",
    "        model.add(Dense(256, input_shape=(self.state_size,), activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # state here is linear list of values for neural network, NOT dictionary\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be472758-8acd-477a-a33b-83ffc07a2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = Tetris_Env()\n",
    "state, _ = env.reset()\n",
    "def convert_state_to_linear(state):\n",
    "    # state is dictionary of observations\n",
    "    input_features =  state[\"board\"].flatten()\n",
    "    input_features = np.append(input_features, [state[\"agent\"][\"x\"], \n",
    "                                                state[\"agent\"][\"y\"],\n",
    "                                                state[\"agent\"][\"piece\"],\n",
    "                                                state[\"agent\"][\"rotation\"],\n",
    "                                                state[\"swap\"],\n",
    "                                                state[\"has_swapped\"]\n",
    "                                               ]\n",
    "                              )\n",
    "\n",
    "    return np.reshape(input_features, [1,len(input_features)])\n",
    "state_size = convert_state_to_linear(state).shape[1]\n",
    "# magic number, \n",
    "# actions = ['no_op', 'left', 'right', 'down', 'cw', 'ccw', 'swap', 'hard']\n",
    "\n",
    "action_size = 8\n",
    "agent = DQNAgent(state_size, action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3f1c8-cd60-4835-92c0-6f70c3d9968b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Iterate the game\n",
    "episodes = 2\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    # reset state in the beginning of each game\n",
    "    # env.reset returns observation, info\n",
    "    state_observation, _ = env.reset()\n",
    "    state = convert_state_to_linear(state_observation)\n",
    "\n",
    "    # time_t represents each frame of the game\n",
    "    #!!!!!!!!!!!!! Fix the number of frames\n",
    "    # print(f'first state has shape {state.shape}')\n",
    "    \n",
    "    max_frames = 5000\n",
    "    for time_t in range(max_frames):\n",
    "        # Decide action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        #  env.step(action) returns: observation, reward, self.game.state == 'gameover', False, info\n",
    "        next_state_obs, reward, done, _, _ = env.step(action) # step makes env make the frame move ahead for us\n",
    "        next_state = convert_state_to_linear(next_state_obs)\n",
    "\n",
    "        # memorize the previous state, action, reward, and done\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "\n",
    "        # done becomes True when the game ends\n",
    "        # ex) The agent drops the pole\n",
    "        if done:\n",
    "            # print the score and break out of the loop\n",
    "            print(f'episode: {e}/{episodes}, reward: {reward}, time = {time_t}')\n",
    "\n",
    "            break\n",
    "\n",
    "    # train the agent with the experience of the episode\n",
    "    agent.replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7515e-810a-4161-ab29-a7ca88dcc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in enumerate([[0] * 10 for _ in range(20)]):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a29fb-f967-4397-b0ac-baa8cd4d7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 0, 5, 5, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 5, 5, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 5, 5, 0, 0, 0], \n",
    "     [0, 0, 0, 6, 5, 5, 0, 0, 0, 0], \n",
    "     [7, 7, 6, 6, 6, 0, 0, 0, 0, 0], \n",
    "     [0, 7, 7, 0, 0, 0, 0, 0, 0, 0], \n",
    "     [0, 5, 0, 0, 0, 0, 0, 0, 1, 0], \n",
    "     [0, 5, 5, 0, 0, 0, 0, 0, 1, 0], \n",
    "     [0, 0, 5, 0, 0, 0, 0, 0, 1, 0], \n",
    "     [0, 0, 4, 4, 0, 5, 5, 0, 1, 0], \n",
    "     [0, 0, 4, 4, 5, 5, 0, 0, 5, 0], \n",
    "     [0, 0, 2, 2, 2, 0, 0, 0, 5, 5], \n",
    "     [0, 0, 0, 0, 2, 0, 0, 7, 7, 5], \n",
    "     [0, 0, 0, 0, 3, 4, 4, 0, 7, 7], \n",
    "     [0, 0, 0, 0, 3, 4, 4, 0, 2, 2], \n",
    "     [0, 0, 0, 1, 3, 3, 0, 0, 2, 0], \n",
    "     [0, 7, 0, 1, 0, 3, 0, 0, 2, 0], \n",
    "     [7, 7, 0, 1, 0, 3, 0, 4, 4, 0], \n",
    "     [7, 0, 0, 1, 0, 3, 3, 4, 4, 0]]\n",
    "x = np.where(np.array(x) != 0, 1, 0)\n",
    "[] + x.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6a1d7-2b4c-4acf-929d-5550c69f1d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346668b-024a-4ca0-bb25-319f354f742e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f0143-1d96-4a23-81c4-9dd199bf8d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c5f65-0ddd-47ea-844b-d58b012aaa24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6dbb4-9508-48b6-80ef-467ab87b593c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e63a63-4b63-46cf-967e-ac5ff0df6a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098adf9-156e-4364-ad7f-ab41ab3233c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d534c-c241-437e-979a-571ba13dc8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2c3d9-e47e-4357-bf89-2258299c8e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8af72-f1c9-4a7e-bcf8-26923d7b8a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0febd-daf3-402f-b161-e0a048b0b8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2fa70d-f59b-48ae-9e70-3be2fe5b52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: \n",
    "# https://github.com/nuno-faria/tetris-ai\n",
    "# tetris_ai.py\n",
    "game = Tetris_Env(render_mode = 'rgb_array')    \n",
    "\n",
    "# when to stop training?\n",
    "episodes = 2000\n",
    "max_steps = None\n",
    "# epsilon_stop_episode = 1500\n",
    "mem_size = 20000\n",
    "discount = 0.95\n",
    "batch_size = 512\n",
    "epochs = 1\n",
    "# render_every = 50\n",
    "log_every = 50\n",
    "replay_start_size = 2000\n",
    "train_every = 1\n",
    "n_neurons = [32, 32]\n",
    "# render_delay = None\n",
    "\n",
    "agent = DQNAgent(env.get_state_size(),\n",
    "                 n_neurons=n_neurons, activations=activations,\n",
    "                 epsilon_stop_episode=epsilon_stop_episode, mem_size=mem_size,\n",
    "                 discount=discount, replay_start_size=replay_start_size)\n",
    "\n",
    "log_dir = f'logs/tetris-nn={str(n_neurons)}-mem={mem_size}-bs={batch_size}-e={epochs}-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "log = CustomTensorBoard(log_dir=log_dir)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    current_state = env.reset()\n",
    "    done = False\n",
    "    steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1579297-b014-4f86-97c1-11e501759c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6de55-23b1-455b-8411-77530892f3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d427b-4525-4f9f-8af8-074efb03e0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51f9f7-18ef-4e96-8941-7f2e5f4d0a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da288d-4de7-49e7-b9e6-50539ad12f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287693ee-6527-4b88-bc62-0866786557c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1a933-4c6c-4514-9c37-4b467b30193c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6baa00f-d26b-4052-99b1-0e73a069e743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38af82-418e-4910-b86e-ae65b183ea76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c7ca9-6b48-4c1c-aea5-edbd33eca47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d6145-3aa2-4f3f-b87b-ea2b159105c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7e261-fecb-46d3-94ff-36ab10e8834d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb230804-0019-4535-bcef-6246f6011ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25ecb8-d862-4a49-9937-bd6d4c71932e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43b80d-4f1f-455e-a6f7-6814d73b31c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc4618-9976-4ee9-ac4d-5c2dd6b28cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d67a8-03bd-428b-8155-196b60d21a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c11f5-4277-436c-9208-c8baae1202ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4519d6f-3f6d-432e-b2eb-d49892f22bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa48d7d-85d6-4f5f-a05c-14f9381784c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a720cc-2d8d-44fd-bf67-9584cc9c4f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394b0d4-8203-4d4c-918c-0720dee05fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d50fe8-c16a-4422-96a2-5ca55118a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Deep Q Learning Agent + Maximin\n",
    "#\n",
    "# This version only provides only value per input,\n",
    "# that indicates the score expected in that state.\n",
    "# This is because the algorithm will try to find the\n",
    "# best final state for the combinations of possible states,\n",
    "# in constrast to the traditional way of finding the best\n",
    "# action for a particular state.\n",
    "class DQNAgent:\n",
    "\n",
    "    '''Deep Q Learning Agent + Maximin\n",
    "    Args:\n",
    "        state_size (int): Size of the input domain\n",
    "        mem_size (int): Size of the replay buffer\n",
    "        discount (float): How important is the future rewards compared to the immediate ones [0,1]\n",
    "        epsilon (float): Exploration (probability of random values given) value at the start\n",
    "        epsilon_min (float): At what epsilon value the agent stops decrementing it\n",
    "        epsilon_stop_episode (int): At what episode the agent stops decreasing the exploration variable\n",
    "        n_neurons (list(int)): List with the number of neurons in each inner layer\n",
    "        activations (list): List with the activations used in each inner layer, as well as the output\n",
    "        loss (obj): Loss function\n",
    "        optimizer (obj): Otimizer used\n",
    "        replay_start_size: Minimum size needed to train\n",
    "    '''\n",
    "\n",
    "    def __init__(self, state_size, mem_size=10000, discount=0.95,\n",
    "                 epsilon=1, epsilon_min=0, epsilon_stop_episode=500,\n",
    "                 n_neurons=[32,32], activations=['relu', 'relu', 'linear'],\n",
    "                 loss='mse', optimizer='adam', replay_start_size=None):\n",
    "\n",
    "        assert len(activations) == len(n_neurons) + 1\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.memory = deque(maxlen=mem_size)\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / (epsilon_stop_episode)\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        if not replay_start_size:\n",
    "            replay_start_size = mem_size / 2\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.model = self._build_model()\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        '''Builds a Keras deep neural network model'''\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.n_neurons[0], input_dim=self.state_size, activation=self.activations[0]))\n",
    "\n",
    "        for i in range(1, len(self.n_neurons)):\n",
    "            model.add(Dense(self.n_neurons[i], activation=self.activations[i]))\n",
    "\n",
    "        model.add(Dense(1, activation=self.activations[-1]))\n",
    "\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def add_to_memory(self, current_state, next_state, reward, done):\n",
    "        '''Adds a play to the replay memory buffer'''\n",
    "        self.memory.append((current_state, next_state, reward, done))\n",
    "\n",
    "\n",
    "    def random_value(self):\n",
    "        '''Random score for a certain action'''\n",
    "        return random.random()\n",
    "\n",
    "\n",
    "    def predict_value(self, state):\n",
    "        '''Predicts the score for a certain state'''\n",
    "        return self.model.predict(state)[0]\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        '''Returns the expected score of a certain state'''\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.random_value()\n",
    "        else:\n",
    "            return self.predict_value(state)\n",
    "\n",
    "\n",
    "    def best_state(self, states):\n",
    "        '''Returns the best state for a given collection of states'''\n",
    "        max_value = None\n",
    "        best_state = None\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.choice(list(states))\n",
    "\n",
    "        else:\n",
    "            for state in states:\n",
    "                value = self.predict_value(np.reshape(state, [1, self.state_size]))\n",
    "                if not max_value or value > max_value:\n",
    "                    max_value = value\n",
    "                    best_state = state\n",
    "\n",
    "        return best_state\n",
    "\n",
    "\n",
    "    def train(self, batch_size=32, epochs=3):\n",
    "        '''Trains the agent'''\n",
    "        n = len(self.memory)\n",
    "    \n",
    "        if n >= self.replay_start_size and n >= batch_size:\n",
    "\n",
    "            batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "            # Get the expected score for the next states, in batch (better performance)\n",
    "            next_states = np.array([x[1] for x in batch])\n",
    "            next_qs = [x[0] for x in self.model.predict(next_states)]\n",
    "\n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "            # Build xy structure to fit the model in batch (better performance)\n",
    "            for i, (state, _, reward, done) in enumerate(batch):\n",
    "                if not done:\n",
    "                    # Partial Q formula\n",
    "                    new_q = reward + self.discount * next_qs[i]\n",
    "                else:\n",
    "                    new_q = reward\n",
    "\n",
    "                x.append(state)\n",
    "                y.append(new_q)\n",
    "\n",
    "            # Fit the model to the given values\n",
    "            self.model.fit(np.array(x), np.array(y), batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "\n",
    "            # Update the exploration variable\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b98094-3ca4-42b1-9206-f361eca8fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.amax([[1,2,3], [3,6,10]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c78e82-c24b-434e-82ca-0846906ec884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a32856-2e23-4bba-923d-4fd4526aded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 0, 5, 5, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 5, 5, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], \n",
    "     [0, 0, 0, 0, 1, 5, 5, 0, 0, 0], \n",
    "     [0, 0, 0, 6, 5, 5, 0, 0, 0, 0], \n",
    "     [7, 7, 6, 6, 6, 0, 0, 0, 0, 0], \n",
    "     [0, 7, 7, 0, 0, 0, 0, 0, 0, 0], \n",
    "     [0, 5, 0, 0, 0, 0, 0, 0, 1, 0], \n",
    "     [0, 5, 5, 0, 0, 0, 0, 0, 1, 0], \n",
    "     [0, 0, 5, 0, 0, 0, 0, 0, 1, 0], \n",
    "     [0, 0, 4, 4, 0, 5, 5, 0, 1, 0], \n",
    "     [0, 0, 4, 4, 5, 5, 0, 0, 5, 0], \n",
    "     [0, 0, 2, 2, 2, 0, 0, 0, 5, 5], \n",
    "     [0, 0, 0, 0, 2, 0, 0, 7, 7, 5], \n",
    "     [0, 0, 0, 0, 3, 4, 4, 0, 7, 7], \n",
    "     [0, 0, 0, 0, 3, 4, 4, 0, 2, 2], \n",
    "     [0, 0, 0, 1, 3, 3, 0, 0, 2, 0], \n",
    "     [0, 7, 0, 1, 0, 3, 0, 0, 2, 0], \n",
    "     [7, 7, 0, 1, 0, 3, 0, 4, 4, 0], \n",
    "     [7, 0, 0, 1, 0, 3, 3, 4, 4, 0]]\n",
    "# x = np.where(np.array(x) != 0, 1, 0)\n",
    "x[:][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be3243-1499-4519-9b32-aca277ac449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34da29d-18e3-4132-9d8e-b90ff2f60e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spaces.Box(low=0, high=1, \n",
    "           shape=(24, 10), \n",
    "           dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bd971-acc4-416f-b85f-6b4753da73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.sample()\n",
    "y, y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bc3ae-5d97-4ebe-8d07-3397ae3dfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[None]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e92298-3cd2-4725-a8c2-7070f612aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "2000000*(2000000-1)/2 - tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81640f0c-1bde-4ecd-9203-ce04354356d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:workingRL2]",
   "language": "python",
   "name": "conda-env-workingRL2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
